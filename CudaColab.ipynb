{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktm3JkI7vrnR"
      },
      "source": [
        "# CUDA on Colab\n",
        "\n",
        "This notebook, based on an example from Nvidia, shows how to check the GPU status of your Colab notebook, check out a github repository containing your c++ code, and compile it using either g++ for CPU or nvcc for GPU. and run it.\n",
        "\n",
        "Not yet covered, profiling.\n",
        "\n",
        "Author: Evelyn Mitchell\n",
        "Source Repository: https://github.com/evelynmitchell/cuda-on-colab\n",
        "Date: 2023-12-04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fsj6QjYwFtx"
      },
      "source": [
        "The nvidia-smi cli tells you about your GPU. The sample outputs for different types of GPUs or TPUs follow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ-Pv2U5IKTJ"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ulMfjWJMUH"
      },
      "source": [
        "A100 GPU\n",
        "```\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpxfIJiUI_c9"
      },
      "source": [
        "V100 GPU\n",
        "```\n",
        "Mon Dec  4 18:42:36 2023       \n",
        "+-----------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
        "|-------------------------------+----------------------+----------------------+\n",
        "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "|                               |                      |               MIG M. |\n",
        "|===============================+======================+======================|\n",
        "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
        "| N/A   31C    P0    23W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
        "|                               |                      |                  N/A |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "                                                                               \n",
        "+-----------------------------------------------------------------------------+\n",
        "| Processes:                                                                  |\n",
        "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
        "|        ID   ID                                                   Usage      |\n",
        "|=============================================================================|\n",
        "|  No running processes found                                                 |\n",
        "+-----------------------------------------------------------------------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tTpRQF-IikI"
      },
      "source": [
        "T4 TPU\n",
        "```\n",
        "Mon Dec  4 18:40:38 2023       \n",
        "+-----------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
        "|-------------------------------+----------------------+----------------------+\n",
        "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "|                               |                      |               MIG M. |\n",
        "|===============================+======================+======================|\n",
        "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
        "| N/A   48C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
        "|                               |                      |                  N/A |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "                                                                               \n",
        "+-----------------------------------------------------------------------------+\n",
        "| Processes:                                                                  |\n",
        "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
        "|        ID   ID                                                   Usage      |\n",
        "|=============================================================================|\n",
        "|  No running processes found                                                 |\n",
        "+-----------------------------------------------------------------------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlfDZjClfzwb"
      },
      "source": [
        "# libcuda Driver\n",
        "\n",
        "If you install some libraries from source, such as Triton, you may lose the libcuda driver which is already installed in colab, when you uninstall triton, to install from source. Following the diagnosis and fix in [6] we will find out if the library is installed, then make sure it is in our execution path.\n",
        "\n",
        "This will show up as:\n",
        "```\n",
        "libcuda.so cannot found\n",
        "```\n",
        "\n",
        "To check if the cuda library is availanble run:\n",
        "```\n",
        "!ldconfig -p |grep libcuda\n",
        "```\n",
        "Which should show a result like\n",
        "```\n",
        "libcudart.so.11.0 (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.11.0\n",
        "libcudart.so (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so\n",
        "```\n",
        "Note that ```libcuda.so``` is not listed.\n",
        "\n",
        "[6] (https://github.com/pytorch/pytorch/issues/107960#issuecomment-1709589190)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koCseLsGfxr7"
      },
      "outputs": [],
      "source": [
        "!ldconfig -p | grep libcuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xo8tBZ9hf01"
      },
      "source": [
        "To find the path to ```libcuda.so``` run\n",
        "```\n",
        "find /usr -name 'libcuda.so'\n",
        "```\n",
        "Which should output something similar to:\n",
        "```\n",
        "/usr/local/cuda-11.8/compat/libcuda.so\n",
        "/usr/local/cuda-11.8/targets/x86_64-linux/lib/stubs/libcuda.so\n",
        "/usr/lib64-nvidia/libcuda.so\n",
        "```\n",
        "The version numbers may be different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHTH84sVhKhL"
      },
      "outputs": [],
      "source": [
        "!find /usr -name 'libcuda.so'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XggaJnTi2B2"
      },
      "source": [
        "We have the same issue as in [6], in that the ```stubs``` path is incorrect, so we will apply the fix, which is to add ```/usr/lib64-nvidia/libcuda.so``` to our shared libraries with:\n",
        "```\n",
        "ldconfig /usr/lib64-nvidia\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrQFIwZfjShm"
      },
      "outputs": [],
      "source": [
        "!ldconfig /usr/lib64-nvidia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxWt0YnijVYT"
      },
      "source": [
        "And  then verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RedH0NsWja0o"
      },
      "outputs": [],
      "source": [
        "!ldconfig -p | grep libcuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y4n7t8Fjiwl"
      },
      "source": [
        "```\n",
        "\tlibcudart.so.11.0 (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.11.0\n",
        "\tlibcudart.so (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so\n",
        "\tlibcudadebugger.so.1 (libc6,x86-64) => /usr/lib64-nvidia/libcudadebugger.so.1\n",
        "\tlibcuda.so.1 (libc6,x86-64) => /usr/lib64-nvidia/libcuda.so.1\n",
        "\tlibcuda.so (libc6,x86-64) => /usr/lib64-nvidia/libcuda.so\n",
        "  ```\n",
        "  that libcuda.so shows up in the list of shared libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN1nWSaHKUVf"
      },
      "source": [
        "# C++ for CUDA\n",
        "Install the c++ build chain, which should be already available on colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKBGPAMsJ4Ji"
      },
      "outputs": [],
      "source": [
        "!apt install build-essential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-VVuTSdwVRJ"
      },
      "source": [
        "The GPU compiler for c++ from Nvidia is called nvcc, and is already installed on Colab, as is build-essential, which provides g++ as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZQLuKHZJ0hW"
      },
      "outputs": [],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjUMM5ygKngM"
      },
      "source": [
        "## Get the code\n",
        "This notebook will show the files inline, and you can also checkout the repository containing the c++ files to compile.\n",
        "```\n",
        "!git clone https://github.com/evelynmitchell/cuda-on-colab\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The simple c++ example of adding the elements of two arrays, without gpu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%file /tmp/simple.cpp\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        "  float *x = new float[N];\n",
        "  float *y = new float[N];\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the CPU\n",
        "  add(N, x, y);\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4LGcpR3Nm91"
      },
      "source": [
        "## Build the code for CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHSDLMN4Nmjg"
      },
      "outputs": [],
      "source": [
        "# compile the code checked out from the repoository to\n",
        "# create the binary /content/cuda-on-colab/src/simple\n",
        "# add the executable bit, and then run it.\n",
        "#!g++ /content/cuda-on-colab/src/simple.cpp -o simple\n",
        "# !chmod +x ./simple\n",
        "# !./simple\n",
        "\n",
        "# compile the code in the cell to create the binary /tmp/simple, \n",
        "#  and then run it.\n",
        "! g++ /tmp/simple.cpp -o /tmp/simple && /tmp/simple\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3myEKLJzShQn"
      },
      "source": [
        "## Compile to a CUDA kernel\n",
        "\n",
        "Adding the  ```__global__``` specifier to a function indicates it will be compiled to a CUDA kernel and run on a GPU processor.\n",
        "\n",
        "This code fails when it's compiled due to an error in how it is called. The error and fix follow this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%file /tmp/simple_cuda.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// CUDA Kernel function to add the elements of two arrays on the GPU\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        "  float *x = new float[N];\n",
        "  float *y = new float[N];\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the CPU\n",
        "  add(N, x, y);\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYgRPRX7Sbuf"
      },
      "outputs": [],
      "source": [
        "# from the repository\n",
        "# !nvcc /content/cuda-on-colab/src/simple_cuda.cu -o simple_cuda && /tmp/simple_cuda\n",
        "\n",
        "# from the cell\n",
        "! nvcc /tmp/simple_cuda.cu -o /tmp/simple_cuda && /tmp/simple_cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0QUba4rrIgO"
      },
      "source": [
        "## Configure kernel launch\n",
        "\n",
        "The error from the prior version of the compilation \"__global__ function call must be configured\" is corrected by adding kernel launch parameters <<<gridsize,blocksize>>> to the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%file /tmp/simple_cuda_kernel_launch.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// CUDA Kernel function to add the elements of two arrays on the GPU\n",
        "\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        "  float *x = new float[N];\n",
        "  float *y = new float[N];\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  // <<< (gridsize), (blocksize) >>>\n",
        "  // <<<1,1>>> means 1 block with 1 thread\n",
        "  add<<<1,1>>>(N, x, y);\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbGjUBb6rGdn"
      },
      "outputs": [],
      "source": [
        "# from the repository\n",
        "# !nvcc /content/cuda-on-colab/src/simple_cuda_kernel_launch.cu -o simple_cuda_kernal_launch\n",
        "\n",
        "# from the cell\n",
        "!nvcc /tmp/simple_cuda_kernel_launch.cu -o /tmp/simple_cuda_kernal_launch && /tmp/simple_cuda_kernal_launch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure kernel threads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%file /tmp/simple_cuda_kernel_threads.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// CUDA Kernel function to add the elements of two arrays on the GPU\n",
        "\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        "  float *x = new float[N];\n",
        "  float *y = new float[N];\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  // <<< (gridsize), (blocksize) >>>\n",
        "  // <<<1,1>>> means 1 block with 1 thread\n",
        "  // \"CUDA GPUs run kernels using blocks of threads that are a multiple of \n",
        "  // 32 in size, so 256 threads is a reasonable size to choose.\"\"\n",
        "  add<<<1,256>>>(N, x, y);\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from the repository\n",
        "# !nvcc /content/cuda-on-colab/src/simple_cuda_kernel_threads.cu -o simple_cuda_kernal_threads\n",
        "\n",
        "# from the cell\n",
        "!nvcc /tmp/simple_cuda_kernel_threads.cu -o /tmp/simple_cuda_kernal_threads && /tmp/simple_cuda_kernal_threads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profile the CUDA code\n",
        "\n",
        "nvprof is the nvidia profiler for CUDA code. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeOa2oOjZYhm"
      },
      "outputs": [],
      "source": [
        "# %cd /content/cuda-on-colab to run from the repository\n",
        "# %cd /tmp to run from the cell\n",
        "!nvprof /tmp/simple_cuda_kernel_launch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory profiling\n",
        "\n",
        "nvprof can also be used to profile memory usage. First we compile an\n",
        "example that uses a lot of memory, then we profile it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%file /tmp/simple_cuda_memory_alloc.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory â€“ accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 1>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "  \n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_fXXIOJiye_"
      },
      "outputs": [],
      "source": [
        "# from the repository\n",
        "# !nvcc /content/cuda-on-colab/src/simple_cuda_memory_alloc.cu -o simple_cuda_memory_alloc\n",
        "\n",
        "# from the cell\n",
        "!nvcc /tmp/simple_cuda_memory_alloc.cu -o /tmp/simple_cuda_memory_alloc && /tmp/simple_cuda_memory_alloc \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ozN41O-jCAi"
      },
      "outputs": [],
      "source": [
        "# run the executable with nvprof\n",
        "!nvprof ./simple_cuda_memory_alloc"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
